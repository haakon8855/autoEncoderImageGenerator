"""Haakon8855"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras as ks

from verification_net import VerificationNet
from stacked_mnist import StackedMNISTData, DataMode


class AutoEncoder(ks.models.Model):
    """
    Auto-encoder class for encoding images as low-dimensional representations.
    """

    def __init__(self,
                 latent_dim,
                 image_size,
                 file_name="./model_encoder/verification_model",
                 retrain=False):
        super(AutoEncoder, self).__init__()
        self.latent_dim = latent_dim
        self.image_size = image_size
        self.file_name = file_name
        self.retrain = retrain

        self.encoder = ks.Sequential([
            ks.Input(shape=(image_size, image_size, 1)),
            ks.layers.Flatten(),
            ks.layers.Dense(512, activation='relu'),
            ks.layers.Dense(32, activation='relu'),
            ks.layers.Dense(latent_dim, activation='relu'),
        ])
        self.encoder.summary()
        self.decoder = ks.Sequential([
            ks.layers.Input(shape=(latent_dim)),
            ks.layers.Dense(32, activation='relu'),
            ks.layers.Dense(256, activation='relu'),
            ks.layers.Dense(512, activation='relu'),
            ks.layers.Dense(image_size**2, activation='sigmoid'),
            ks.layers.Reshape((image_size, image_size))
        ])
        self.decoder.summary()
        self.done_training = self.load_all_weights()

    def load_all_weights(self):
        """
        Load weights
        """
        # noinspection PyBroadException
        if self.retrain:
            return False
        try:
            self.load_weights(filepath=self.file_name)
            # print(f"Read model from file, so I do not retrain")
            done_training = True
        except:  # pylint: disable=bare-except
            print(
                "Could not read weights for verification_net from file. Must retrain..."
            )
            done_training = False

        return done_training

    def train(self, x_train, epochs, batch_size, shuffle, validation_data):
        """
        Train the auto-encoder
        """
        self.done_training = self.load_all_weights()

        if not self.done_training or self.retrain:
            self.fit(x_train,
                     x_train,
                     epochs=epochs,
                     batch_size=batch_size,
                     shuffle=shuffle,
                     validation_data=validation_data)
            self.save_weights(filepath=self.file_name)
            self.done_training = True

    def call(self, x_input):
        """
        Return network output given an input x_input.
        """
        encoded = self.encoder(x_input)
        decoded = self.decoder(encoded)
        return decoded

    def generate_images(self, number_to_generate):
        """
        Generate a number of images by generating random vectors in the latent
        vector space and feeding them through the decoder.
        """
        latent_vectors = np.random.randn(number_to_generate, self.latent_dim)
        return self.decoder(latent_vectors).numpy()

    def measure_loss(self, x_test, reconstruced, check_range=200):
        """
        Measures the loss for each test sample and returns a list of losses
        corresponding to each sample in x_test on the same index.
        """
        loss = []
        for i in range(check_range):
            x_true = x_test[i, :, :, :][np.newaxis, :, :, :]
            x_pred = reconstruced[:, :, :,
                                  np.newaxis][i, :, :, :][np.newaxis, :, :, :]
            loss.append(self.evaluate(x_true, x_pred, verbose=0))
        return loss

    def display_generated(self, images, amount_generated):
        """
        Display the images generated by the generative model.
        """
        plt.figure(figsize=(amount_generated, 4))
        for i in range(amount_generated):
            # Display generative
            ax = plt.subplot(2, amount_generated, i + 1 + amount_generated)
            plt.imshow(images[i])
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.show()

    def display_anomalies(self, k, x_test, x_pred, loss):
        """
        Display the k most anomalous image reconstructions found in
        the test set.
        """
        largest_loss = np.argpartition(loss, -k)[-k:]
        plt.figure(figsize=(k, k))
        for i in range(k):
            # Display original
            ax = plt.subplot(2, k, i + 1)
            plt.imshow(x_test[largest_loss[i]])
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
            # Display most loss
            ax = plt.subplot(2, k, i + k + 1)
            plt.imshow(x_pred[largest_loss[i]])
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.show()

    def display_reconstructions(self, x_test, x_pred, n=20, offset=0):
        """
        Display n original images along with their reconstruction by the AE
        """
        plt.figure(figsize=(20, 4))
        for i in range(n):
            # Display original
            ax = plt.subplot(2, n, i + 1)
            plt.imshow(x_test[i + offset])
            plt.title(i + 1)
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
            # Display reconstruction
            ax = plt.subplot(2, n, i + 1 + n)
            plt.imshow(x_pred[i + offset])
            plt.title(i + 1)
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.show()


def main():
    """
    Main function for running the auto encoder.
    """
    # Modifiable parameters
    latent_dim = 5
    epochs = 30
    image_size = 28
    retrain = False
    run_anomaly_detection = True
    batch_size = 1024
    # Anomaly detection
    check_for_anomalies = 1000
    k_anomalies = 10
    # Reconstruction display
    number_of_reconstructions = 20
    display_offset = 60
    # Generative model
    number_to_generate = 20

    # Set up data set generators/fetchers
    data_mode = DataMode.MONO_BINARY_COMPLETE
    if run_anomaly_detection:
        data_mode = DataMode.MONO_BINARY_MISSING
    gen_train = StackedMNISTData(mode=data_mode, default_batch_size=2048)
    gen_test = StackedMNISTData(mode=data_mode, default_batch_size=2048)

    # Fetch training and test data sets
    x_train, _ = gen_train.get_full_data_set(training=True)
    x_test, y_test = gen_test.get_full_data_set(training=False)
    # Only look at one channel:
    x_train = x_train[:, :, :, [0]]
    x_test = x_test[:, :, :, [0]]

    # Train the verification net (will use stored weights as long as
    # force_learn=False)
    net = VerificationNet(force_learn=False)
    net.train(generator=gen_test, epochs=5)

    # Initialize the auto encoder with size of latent representation,
    # the image size along one axis, and whether to retrain the network
    # or use the stored weights.
    auto_encoder = AutoEncoder(latent_dim, image_size, retrain=retrain)
    auto_encoder.compile(
        optimizer='adam',
        loss=ks.losses.BinaryCrossentropy(),
    )
    # Train the network using the training data set and predefined parameters
    auto_encoder.train(x_train,
                       epochs=epochs,
                       batch_size=batch_size,
                       shuffle=True,
                       validation_data=(x_test, x_test))

    if run_anomaly_detection:
        gen_test = StackedMNISTData(mode=DataMode.MONO_BINARY_COMPLETE,
                                    default_batch_size=2048)

        # Fetch complete test data set
        x_test, y_test = gen_test.get_full_data_set(training=False)
        # Only look at one channel:
        x_test = x_test[:, :, :, [0]]

    encoded_imgs = auto_encoder.encoder(x_test).numpy()
    decoded_imgs = auto_encoder.decoder(encoded_imgs).numpy()

    if run_anomaly_detection:
        print("Checking for anomalies")
        loss = auto_encoder.measure_loss(x_test,
                                         decoded_imgs,
                                         check_range=check_for_anomalies)
        # Display the k_anomalies images with the most loss for
        # first 'check_for_anomalies' (e.g. 1000) samples of test set.
        auto_encoder.display_anomalies(k_anomalies, x_test, decoded_imgs, loss)

    print(y_test[display_offset:number_of_reconstructions + display_offset])
    auto_encoder.display_reconstructions(x_test, decoded_imgs,
                                         number_of_reconstructions,
                                         display_offset)

    # Generate random vectors in the latent vector-space and feed them
    # through the decoder.
    generated = auto_encoder.generate_images(number_to_generate)

    auto_encoder.display_generated(generated, number_to_generate)

    # Send reconstructed images through the verification net
    shape = decoded_imgs.shape

    # Check coverage, predictability and accuracy for the reconstructed
    # original images.
    cov = net.check_class_coverage(
        decoded_imgs.reshape(shape[0], shape[1], shape[2], 1))
    pred, acc = net.check_predictability(
        decoded_imgs.reshape(shape[0], shape[1], shape[2], 1), y_test)

    print(f"Coverage: {100*cov:.2f}%")
    print(f"Predictability: {100*pred:.2f}%")
    print(f"Accuracy: {100 * acc:.2f}%")


if __name__ == "__main__":
    main()
